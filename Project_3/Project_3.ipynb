{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3\n",
    "\n",
    "## Getting Insights into Images and their Metadata\n",
    "\n",
    "\n",
    "Analysis will include:\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import scipy\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "import sqlite3\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/katy/anaconda3/envs/mlds/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/katy/anaconda3/envs/mlds/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "vgg16 = torchvision.models.vgg16(pretrained=True) # very slow command"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "funny, for me that command took maybe half a second? /sara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 8, 8])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vgg16.features is a series of modules.\n",
    "# we can chain an input through the modules via the forward method.\n",
    "phi = vgg16.features.forward\n",
    "\n",
    "def preprocess_image(img):\n",
    "\n",
    "    # commented out lines are (probably) unnecessary? I hope?\n",
    "    transform = transforms.Compose([\n",
    "        #transforms.Resize(256), # or 224\n",
    "        #transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    batch = transform(img)\n",
    "    # batch = torch.unsqueeze(batch, 0)\n",
    "    return batch \n",
    "\n",
    "def fetch_img(img_index:int, healthy:bool=True):\n",
    "    if healthy:\n",
    "        return Image.open(f\"Apple___healthy/image ({img_index}).JPG\")\n",
    "    else:\n",
    "        return Image.open(f\"Apple___Black_rot/image ({img_index}).JPG\")\n",
    "\n",
    "def img_features(img_index:int, healthy:bool=True, flatten=False):\n",
    "    res = phi(preprocess_image(fetch_img(img_index, healthy=healthy)))\n",
    "    if flatten:\n",
    "        res = torch.flatten(res)\n",
    "    return res\n",
    "\n",
    "phi_out_shape = img_features(1, healthy=True).shape\n",
    "phi_out_shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The guessing process consists of the following pipeline of functions:\n",
    "\n",
    "fetch_img -> preprocess_image -> phi (aka our neural network) -> flatten -> guess\n",
    "\n",
    "where guess requires w to be calculated, which is the normalized flattened difference of mean phi outputs of healthy and unhealthy images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to calculate the means for both healthy and unhealthy image features\n",
    "def mean_features(indices, healthy=True, flatten = True):\n",
    "    mean = None\n",
    "    for x in indices:\n",
    "        if mean is None:\n",
    "            mean = img_features(x, healthy=healthy, flatten=flatten)\n",
    "        else:\n",
    "            mean += img_features(x, healthy=healthy, flatten=flatten)\n",
    "\n",
    "    return mean / len(indices)\n",
    "\n",
    "training_indices = range(1, 20) # first few images as training set. Can always be changed later.\n",
    "\n",
    "mean_healthy = mean_features(training_indices, healthy=True)\n",
    "mean_rotted = mean_features(training_indices, healthy=False)\n",
    "\n",
    "w = mean_rotted - mean_healthy\n",
    "w = w / torch.linalg.norm(w) \n",
    "\n",
    "def guess(x): # takes a flattened tensor of image features\n",
    "    return torch.dot(w, x) # magnitude of projection onto line from mean to mean\n",
    "\n",
    "def img_guess(img_index, healthy=True): \n",
    "    return guess(img_features(img_index, healthy, flatten=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to test the guessing with some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.8400, grad_fn=<AbsBackward0>)\n",
      "tensor(2.5386, grad_fn=<AbsBackward0>)\n",
      "tensor(34.9783, grad_fn=<AbsBackward0>)\n",
      "tensor(32.6769, grad_fn=<AbsBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# see if we can correctly identify the first healthy/unhealthy leaves not in the training set\n",
    "test_healthy = img_guess(21, healthy=True)\n",
    "test_rotted = img_guess(21, healthy=False)\n",
    "\n",
    "mean_healthy_position = guess(mean_healthy)\n",
    "mean_rotted_position = guess(mean_rotted)\n",
    "# smaller values imply higher confidence\n",
    "true_neg = torch.abs(test_healthy - mean_healthy_position)\n",
    "true_pos = torch.abs(test_rotted - mean_rotted_position)\n",
    "false_neg = torch.abs(test_rotted - mean_healthy_position)\n",
    "false_pos = torch.abs(test_healthy - mean_rotted_position)\n",
    "print(true_neg)\n",
    "print(true_pos)\n",
    "print(false_neg)\n",
    "print(false_pos)\n",
    "# smaller values imply higher confidence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interpretation: Our model can quite accurately determine if the image is rotted based on just 20 images of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
