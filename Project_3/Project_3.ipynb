{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3\n",
    "\n",
    "## Getting Insights into Images and their Metadata\n",
    "\n",
    "\n",
    "Analysis will include:\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import scipy\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "from sklearn import metrics\n",
    "import sqlite3\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saraw\\miniconda3\\envs\\mlfds\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saraw\\miniconda3\\envs\\mlfds\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "vgg16 = torchvision.models.vgg16(pretrained=True) # very slow command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "funny, for me that command took maybe half a second? /sara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 8, 8])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vgg16.features is a series of modules.\n",
    "# we can chain an input through the modules via the forward method.\n",
    "phi = vgg16.features.forward\n",
    "\n",
    "def preprocess_image(img):\n",
    "\n",
    "    # commented out lines are (probably) unnecessary? I hope?\n",
    "    transform = transforms.Compose([\n",
    "        #transforms.Resize(256), # or 224\n",
    "        #transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    batch = transform(img)\n",
    "    # batch = torch.unsqueeze(batch, 0)\n",
    "    return batch \n",
    "\n",
    "def fetch_img(img_index:int, healthy:bool=True):\n",
    "    if healthy:\n",
    "        return Image.open(f\"Apple___healthy/image ({img_index}).JPG\")\n",
    "    else:\n",
    "        return Image.open(f\"Apple___Black_rot/image ({img_index}).JPG\")\n",
    "\n",
    "def img_features(img_index:int, healthy:bool=True, flatten=False):\n",
    "    res = phi(preprocess_image(fetch_img(img_index, healthy=healthy)))\n",
    "    if flatten:\n",
    "        res = torch.flatten(res)\n",
    "    return res\n",
    "\n",
    "phi_out_shape = img_features(1, healthy=True).shape\n",
    "phi_out_shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The guessing process consists of the following pipeline of functions:\n",
    "\n",
    "fetch_img -> preprocess_image -> phi (aka our neural network) -> flatten -> guess\n",
    "\n",
    "where guess requires w to be calculated, which is the normalized flattened difference of mean phi outputs of healthy and unhealthy images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to calculate the means for both healthy and unhealthy image features\n",
    "def mean_features(indices, healthy=True, flatten = True):\n",
    "    mean = None\n",
    "    for x in indices:\n",
    "        if mean is None:\n",
    "            mean = img_features(x, healthy=healthy, flatten=flatten)\n",
    "        else:\n",
    "            mean += img_features(x, healthy=healthy, flatten=flatten)\n",
    "\n",
    "    return mean / len(indices)\n",
    "\n",
    "training_indices = range(1, 50) # first few images as training set. Can always be changed later. (Now)\n",
    "\n",
    "mean_healthy = mean_features(training_indices, healthy=True)\n",
    "mean_rotted = mean_features(training_indices, healthy=False)\n",
    "\n",
    "w = mean_rotted - mean_healthy\n",
    "w = w / torch.linalg.norm(w) \n",
    "\n",
    "def guess(x): # takes a flattened tensor of image features\n",
    "    return torch.dot(w, x) # magnitude of projection onto line from mean to mean\n",
    "\n",
    "def img_guess(img_index, healthy=True): \n",
    "    return guess(img_features(img_index, healthy, flatten=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to test the guessing with some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.2134, grad_fn=<AbsBackward0>)\n",
      "tensor(5.3685, grad_fn=<AbsBackward0>)\n",
      "tensor(40.4772, grad_fn=<AbsBackward0>)\n",
      "tensor(31.8954, grad_fn=<AbsBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# see if we can correctly identify the first healthy/unhealthy leaves not in the training set\n",
    "test_healthy = img_guess(51, healthy=True)\n",
    "test_rotted = img_guess(51, healthy=False)\n",
    "\n",
    "mean_healthy_position = guess(mean_healthy)\n",
    "mean_rotted_position = guess(mean_rotted)\n",
    "# smaller values imply higher confidence\n",
    "true_neg = torch.abs(test_healthy - mean_healthy_position)\n",
    "true_pos = torch.abs(test_rotted - mean_rotted_position)\n",
    "false_neg = torch.abs(test_rotted - mean_healthy_position)\n",
    "false_pos = torch.abs(test_healthy - mean_rotted_position)\n",
    "print(true_neg)\n",
    "print(true_pos)\n",
    "print(false_neg)\n",
    "print(false_pos)\n",
    "# smaller values imply higher confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interpretation: Our model can quite accurately determine if the image is rotted based on just 20 images of training data. (now 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of ROC:  0.9\n"
     ]
    }
   ],
   "source": [
    "#write some kind of decision function\n",
    "\n",
    "test_size = 10 # classify 10 random images that were not in the test set\n",
    "true_labels = np.tile(np.array([1, 0]), test_size)\n",
    "estimated = []\n",
    "\n",
    "for t in range(test_size):\n",
    "    # rotten has exactly 1000 entrys\n",
    "    # healty has 1645 or so pictures, i don't know for sure though (I'm dyslexic, could be anything like 1456 or 1654 or whatever)\n",
    "    # so lets just say 1000\n",
    "    rand_test = np.random.randint(51, 1000)\n",
    "    \n",
    "    test_healthy = img_guess(rand_test, healthy=True)\n",
    "    test_rotted = img_guess(rand_test, healthy=False)\n",
    "    # decide on label (I know its ugly code but I'm tired for now)\n",
    "    if torch.abs(test_healthy - mean_healthy_position) < torch.abs(test_healthy - mean_rotted_position):\n",
    "        estimated.append(1)\n",
    "    else:\n",
    "        estimated.append(0)\n",
    "    if torch.abs(test_rotted - mean_rotted_position) < torch.abs(test_rotted - mean_healthy_position):\n",
    "        estimated.append(0)\n",
    "    else:\n",
    "        estimated.append(1)\n",
    "\n",
    "print('AUC of ROC: ', metrics.roc_auc_score(true_labels, estimated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weee, thats decent enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m preprocess_image(fetch_img(\u001b[38;5;241m21\u001b[39m, healthy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m      4\u001b[0m x\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\mlfds\\lib\\site-packages\\torch\\autograd\\__init__.py:300\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior."
     ]
    }
   ],
   "source": [
    "# something something sensitivity\n",
    "y = torch.mean(img_features(21, healthy=True, flatten=True)) # take mean so its a scalar\n",
    "x = preprocess_image(fetch_img(21, healthy=True))\n",
    "x.requires_grad = True\n",
    "torch.autograd.grad(y, x, create_graph=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
