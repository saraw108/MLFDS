{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3\n",
    "\n",
    "## Getting Insights into Images and their Metadata\n",
    "\n",
    "\n",
    "Analysis will include:\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import scipy\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "import sqlite3\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saraw\\miniconda3\\envs\\mlfds\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saraw\\miniconda3\\envs\\mlfds\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "vgg16 = torchvision.models.vgg16(pretrained=True) # very slow command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "funny, for me that command took maybe half a second? /sara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 8, 8])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vgg16.features is a series of modules.\n",
    "# we can chain an input through the modules via the forward method.\n",
    "phi = vgg16.features.forward\n",
    "\n",
    "def preprocess_image(img):\n",
    "\n",
    "    # commented out lines are (probably) unnecessary? I hope?\n",
    "    transform = transforms.Compose([\n",
    "        #transforms.Resize(256), # or 224\n",
    "        #transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    batch = transform(img)\n",
    "    # batch = torch.unsqueeze(batch, 0)\n",
    "    return batch \n",
    "\n",
    "def fetch_img(img_index:int, healthy:bool=True):\n",
    "    if healthy:\n",
    "        return Image.open(f\"Apple___healthy/image ({img_index}).JPG\")\n",
    "    else:\n",
    "        return Image.open(f\"Apple___Black_rot/image ({img_index}).JPG\")\n",
    "\n",
    "def img_features(img_index:int, healthy:bool=True):\n",
    "    return phi(preprocess_image(fetch_img(img_index, healthy=healthy)))\n",
    "\n",
    "phi_out_shape = img_features(1, healthy=True).shape\n",
    "phi_out_shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "actually I think centercrop and resizing to the size the training data had is relativly common but i don't know if thats neccessary here, I guess it wouldnt work otherwise? /sara <br> anyway, I will also reshape it nevertheless so its 1D and the we'll see /sara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 6.5840e-01,  5.8461e-01,  7.9042e-01,  1.3707e+00,  1.6708e+00,\n",
       "           9.6479e-01,  7.9480e-01,  1.8551e+00],\n",
       "         [ 3.0120e-01, -9.8316e-02, -7.9643e-01, -2.9018e-01,  8.7823e-01,\n",
       "           7.2210e-01,  9.0211e-01,  1.1072e+00],\n",
       "         [-3.3040e-02, -1.1047e+00, -2.0431e+00, -1.6285e+00, -8.1169e-01,\n",
       "          -6.4916e-01, -1.3468e-03,  2.5760e-01],\n",
       "         [-2.6506e-01, -1.3277e+00, -2.1925e+00, -1.7456e+00, -1.1781e+00,\n",
       "          -8.9657e-01, -1.8042e-01, -7.4197e-02],\n",
       "         [ 1.2060e-01,  9.2153e-02, -3.5975e-01, -3.1272e-01, -2.0766e-01,\n",
       "          -2.7387e-01,  1.2102e-01,  3.2935e-01],\n",
       "         [ 8.8628e-01, -5.6385e-02, -6.7700e-01, -4.5892e-01, -3.0320e-01,\n",
       "          -5.9503e-01, -1.0174e-01,  8.9331e-01],\n",
       "         [ 1.8325e-01, -2.1189e+00, -2.5124e+00, -1.6861e+00, -1.1622e+00,\n",
       "          -1.3821e+00, -1.0510e+00,  2.6827e-01],\n",
       "         [-1.6990e-01, -7.8924e-01, -5.1853e-01,  1.4417e-02, -1.2932e-01,\n",
       "          -2.1154e-01, -1.7538e-01,  9.4159e-02]],\n",
       "        grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[ 6.5840e-01,  5.8461e-01,  7.9042e-01,  1.3707e+00,  1.6708e+00,\n",
       "           9.6479e-01,  7.9480e-01,  1.8551e+00],\n",
       "         [ 3.0120e-01, -9.8316e-02, -7.9643e-01, -2.9018e-01,  8.7823e-01,\n",
       "           7.2210e-01,  9.0211e-01,  1.1072e+00],\n",
       "         [-3.3040e-02, -1.1047e+00, -2.0431e+00, -1.6285e+00, -8.1169e-01,\n",
       "          -6.4916e-01, -1.3468e-03,  2.5760e-01],\n",
       "         [-2.6506e-01, -1.3277e+00, -2.1925e+00, -1.7456e+00, -1.1781e+00,\n",
       "          -8.9657e-01, -1.8042e-01, -7.4197e-02],\n",
       "         [ 1.2060e-01,  9.2153e-02, -3.5975e-01, -3.1272e-01, -2.0766e-01,\n",
       "          -2.7387e-01,  1.2102e-01,  3.2935e-01],\n",
       "         [ 8.8628e-01, -5.6385e-02, -6.7700e-01, -4.5892e-01, -3.0320e-01,\n",
       "          -5.9503e-01, -1.0174e-01,  8.9331e-01],\n",
       "         [ 1.8325e-01, -2.1189e+00, -2.5124e+00, -1.6861e+00, -1.1622e+00,\n",
       "          -1.3821e+00, -1.0510e+00,  2.6827e-01],\n",
       "         [-1.6990e-01, -7.8924e-01, -5.1853e-01,  1.4417e-02, -1.2932e-01,\n",
       "          -2.1154e-01, -1.7538e-01,  9.4159e-02]],\n",
       "        grad_fn=<ReshapeAliasBackward0>))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean_features(indices, healthy=True):\n",
    "    mean = None\n",
    "    for x in indices:\n",
    "        if mean is None:\n",
    "            mean = img_features(x, healthy=healthy)\n",
    "        mean += img_features(x, healthy=healthy)\n",
    "    return mean\n",
    "\n",
    "training_indices = range(1, 6) # first few images as training set. Can always be changed later.\n",
    "\n",
    "mean_healthy = mean_features(training_indices, healthy=True)\n",
    "mean_rotted = mean_features(training_indices, healthy=False)\n",
    "\n",
    "w = mean_rotted - mean_healthy\n",
    "w = torch.transpose( w / torch.linalg.norm(w), 0, 1 )\n",
    "\n",
    "def guess(x):\n",
    "    return torch.tensordot(w, phi(x))\n",
    "\n",
    "def img_guess(img_index, healthy=True): # g(x) from the project3 pdf\n",
    "    return torch.tensordot(w, img_features(img_index, healthy=True))\n",
    "\n",
    "# see if we can correctly identify the first healthy/unhealthy leaves not in the training set\n",
    "test_healthy = img_guess(11, healthy=True)\n",
    "test_unhealthy = img_guess(11, healthy=False)\n",
    "\n",
    "test_healthy, test_unhealthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
